{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689,"modelId":2821}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":4.620973,"end_time":"2024-07-16T18:37:45.361288","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-16T18:37:40.740315","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install keras_core\n! pip install text_hammer ","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:43:28.124303Z","iopub.execute_input":"2024-07-28T15:43:28.125115Z","iopub.status.idle":"2024-07-28T15:44:11.113324Z","shell.execute_reply.started":"2024-07-28T15:43:28.125081Z","shell.execute_reply":"2024-07-28T15:44:11.112374Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: keras_nlp in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2023.12.25)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (13.7.0)\nRequirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (0.2.7)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras_nlp) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras_nlp) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras_nlp) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.17.2)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (2.15.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras_nlp) (2.15.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras_nlp) (2024.7.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\nCollecting abbreviations\n  Downloading abbreviations-0.2.5-py3-none-any.whl.metadata (550 bytes)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from abbreviations) (2023.12.25)\nDownloading abbreviations-0.2.5-py3-none-any.whl (5.7 kB)\nInstalling collected packages: abbreviations\nSuccessfully installed abbreviations-0.2.5\nCollecting text_hammer\n  Downloading text_hammer-0.1.5-py3-none-any.whl.metadata (2.2 kB)\nCollecting beautifulsoup4==4.9.1 (from text_hammer)\n  Downloading beautifulsoup4-4.9.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from text_hammer) (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from text_hammer) (1.26.4)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from text_hammer) (3.7.5)\nRequirement already satisfied: TextBlob in /opt/conda/lib/python3.10/site-packages (from text_hammer) (0.18.0.post0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4==4.9.1->text_hammer) (2.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->text_hammer) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->text_hammer) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->text_hammer) (2023.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->text_hammer) (3.4.0)\nCollecting nltk>=3.8 (from TextBlob->text_hammer)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->text_hammer) (1.2.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->TextBlob->text_hammer) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->TextBlob->text_hammer) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->TextBlob->text_hammer) (2023.12.25)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy->text_hammer) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->text_hammer) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->text_hammer) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->text_hammer) (4.9.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->text_hammer) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->text_hammer) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->text_hammer) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->text_hammer) (0.1.4)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->text_hammer) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy->text_hammer) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy->text_hammer) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy->text_hammer) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy->text_hammer) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->text_hammer) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->text_hammer) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->text_hammer) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->text_hammer) (0.1.2)\nDownloading text_hammer-0.1.5-py3-none-any.whl (7.6 kB)\nDownloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk, beautifulsoup4, text_hammer\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.12.2\n    Uninstalling beautifulsoup4-4.12.2:\n      Successfully uninstalled beautifulsoup4-4.12.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibpysal 4.9.2 requires beautifulsoup4>=4.10, but you have beautifulsoup4 4.9.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed beautifulsoup4-4.9.1 nltk-3.8.1 text_hammer-0.1.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.97955,"end_time":"2024-07-16T18:37:44.837219","exception":false,"start_time":"2024-07-16T18:37:43.857669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-28T15:46:32.329962Z","iopub.execute_input":"2024-07-28T15:46:32.330338Z","iopub.status.idle":"2024-07-28T15:46:32.380682Z","shell.execute_reply.started":"2024-07-28T15:46:32.330307Z","shell.execute_reply":"2024-07-28T15:46:32.379478Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_core\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay, confusion_matrix\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_core'"],"ename":"ModuleNotFoundError","evalue":"No module named 'keras_core'","output_type":"error"}]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:20:11.894698Z","iopub.execute_input":"2024-07-28T15:20:11.895198Z","iopub.status.idle":"2024-07-28T15:20:11.989413Z","shell.execute_reply.started":"2024-07-28T15:20:11.895150Z","shell.execute_reply":"2024-07-28T15:20:11.988321Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install abbreviations_py","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:30:18.664370Z","iopub.execute_input":"2024-07-28T15:30:18.665331Z","iopub.status.idle":"2024-07-28T15:30:33.689450Z","shell.execute_reply.started":"2024-07-28T15:30:18.665302Z","shell.execute_reply":"2024-07-28T15:30:33.688386Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting abbreviations_py\n  Downloading abbreviations_py-0.2.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: abbreviations_py\n  Building wheel for abbreviations_py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for abbreviations_py: filename=abbreviations_py-0.2-py3-none-any.whl size=2942 sha256=f67e195710ca1feb4b6bcd6bbd1b1a6c9b853c6d1e319418440a3db0f5d1465b\n  Stored in directory: /root/.cache/pip/wheels/0e/94/39/9d0b4fec8e8700d958c1647e1f0c080b97630d63ecab784481\nSuccessfully built abbreviations_py\nInstalling collected packages: abbreviations_py\nSuccessfully installed abbreviations_py-0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n\ntqdm.pandas()\n\ndef preprocess_tweets(df, text_column):\n    \n    # Lowercase the text\n    df[text_column] = df[text_column].progress_apply(lambda x: str(x).lower())\n    \n    # Remove emails\n    df[text_column] = df[text_column].progress_apply(lambda x: th.remove_emails(x))\n    \n    # Remove HTML tags\n    df[text_column] = df[text_column].progress_apply(lambda x: th.remove_html_tags(x))\n    \n    # Remove special characters\n    df[text_column] = df[text_column].progress_apply(lambda x: th.remove_special_chars(x))\n    \n    # Remove accented characters\n    df[text_column] = df[text_column].progress_apply(lambda x: th.remove_accented_chars(x))\n    \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:34:01.754653Z","iopub.execute_input":"2024-07-28T15:34:01.755526Z","iopub.status.idle":"2024-07-28T15:34:01.762930Z","shell.execute_reply.started":"2024-07-28T15:34:01.755493Z","shell.execute_reply":"2024-07-28T15:34:01.762055Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_train = preprocess_tweets(df_train, \"text\")\ndf_test = preprocess_tweets(df_test, \"text\")\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:34:04.511967Z","iopub.execute_input":"2024-07-28T15:34:04.512627Z","iopub.status.idle":"2024-07-28T15:34:06.910221Z","shell.execute_reply.started":"2024-07-28T15:34:04.512595Z","shell.execute_reply":"2024-07-28T15:34:06.909311Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"100%|██████████| 7613/7613 [00:00<00:00, 459508.37it/s]\n100%|██████████| 7613/7613 [00:00<00:00, 91740.08it/s]\n100%|██████████| 7613/7613 [00:01<00:00, 5311.43it/s]\n100%|██████████| 7613/7613 [00:00<00:00, 108599.67it/s]\n100%|██████████| 7613/7613 [00:00<00:00, 323488.60it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 429311.27it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 88856.95it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 5087.15it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 107255.48it/s]\n100%|██████████| 3263/3263 [00:00<00:00, 282973.51it/s]\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n1   4     NaN      NaN              forest fire near la ronge sask canada   \n2   5     NaN      NaN  all residents asked to shelter in place are be...   \n3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>our deeds are the reason of this earthquake ma...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>all residents asked to shelter in place are be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>just got sent this photo from ruby alaska as s...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# Download the VADER lexicon\n# nltk.download('vader_lexicon')\n\ndef sentiment(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sentiment_scores = analyzer.polarity_scores(text)\n    compound_score = sentiment_scores['compound']\n    \n    if compound_score >= 0.05:\n        return 'positive'\n    elif compound_score <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf_train['sentiment'] = df_train['text'].apply(sentiment)\ndf_test['sentiment'] = df_test['text'].apply(sentiment)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:34:16.061525Z","iopub.execute_input":"2024-07-28T15:34:16.061900Z","iopub.status.idle":"2024-07-28T15:35:46.073046Z","shell.execute_reply.started":"2024-07-28T15:34:16.061869Z","shell.execute_reply":"2024-07-28T15:35:46.072037Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Ensure all columns are strings before concatenation\ndf_train[\"text\"] = df_train[\"text\"].astype(str) + \" \" + df_train[\"keyword\"].astype(str) + \" \" + df_train[\"sentiment\"].astype(str)\n\n# Display the updated 'text' column\ndf_train[\"text\"].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:27.445562Z","iopub.execute_input":"2024-07-28T15:36:27.446168Z","iopub.status.idle":"2024-07-28T15:36:27.460861Z","shell.execute_reply.started":"2024-07-28T15:36:27.446137Z","shell.execute_reply":"2024-07-28T15:36:27.459769Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0    our deeds are the reason of this earthquake ma...\n1    forest fire near la ronge sask canada nan nega...\n2    all residents asked to shelter in place are be...\n3    13000 people receive wildfires evacuation orde...\n4    just got sent this photo from ruby alaska as s...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Ensure all columns are strings before concatenation\ndf_test[\"text\"] = df_test[\"text\"].astype(str) + \" \" + df_test[\"keyword\"].astype(str) + \" \" + df_test[\"sentiment\"].astype(str)\n\n# Display the updated 'text' column\ndf_test[\"text\"].head()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:29.938134Z","iopub.execute_input":"2024-07-28T15:36:29.938496Z","iopub.status.idle":"2024-07-28T15:36:29.950289Z","shell.execute_reply.started":"2024-07-28T15:36:29.938464Z","shell.execute_reply":"2024-07-28T15:36:29.949154Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0      just happened a terrible car crash nan negative\n1    heard about earthquake is different cities sta...\n2    there is a forest fire at spot pond geese are ...\n3    apocalypse lighting spokane wildfires nan neutral\n4    typhoon soudelor kills 28 in china and taiwan ...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:32.098882Z","iopub.execute_input":"2024-07-28T15:36:32.099881Z","iopub.status.idle":"2024-07-28T15:36:32.129691Z","shell.execute_reply.started":"2024-07-28T15:36:32.099838Z","shell.execute_reply":"2024-07-28T15:36:32.128660Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Train Length Stat\ncount    7613.000000\nmean      111.927361\nstd        32.397660\nmin        18.000000\n25%        89.000000\n50%       117.000000\n75%       141.000000\nmax       169.000000\nName: length, dtype: float64\n\nTest Length Stat\ncount    3263.000000\nmean      113.065584\nstd        32.500040\nmin        19.000000\n25%        91.000000\n50%       120.000000\n75%       142.000000\nmax       171.000000\nName: length, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:56.895009Z","iopub.execute_input":"2024-07-28T15:36:56.895591Z","iopub.status.idle":"2024-07-28T15:36:56.900516Z","shell.execute_reply.started":"2024-07-28T15:36:56.895560Z","shell.execute_reply":"2024-07-28T15:36:56.899526Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train[\"text\"]\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = df_test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:59.981638Z","iopub.execute_input":"2024-07-28T15:36:59.982036Z","iopub.status.idle":"2024-07-28T15:36:59.990764Z","shell.execute_reply.started":"2024-07-28T15:36:59.982007Z","shell.execute_reply":"2024-07-28T15:36:59.989795Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"! pip install tensorflow-text","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:38:44.052665Z","iopub.execute_input":"2024-07-28T15:38:44.053679Z","iopub.status.idle":"2024-07-28T15:38:56.734023Z","shell.execute_reply.started":"2024-07-28T15:38:44.053640Z","shell.execute_reply":"2024-07-28T15:38:56.733025Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text) (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text) (2.15.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)\n\nclassifier.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:39:05.566559Z","iopub.execute_input":"2024-07-28T15:39:05.567467Z","iopub.status.idle":"2024-07-28T15:39:09.219987Z","shell.execute_reply.started":"2024-07-28T15:39:05.567428Z","shell.execute_reply":"2024-07-28T15:39:09.218607Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py:234\u001b[0m, in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/distil_bert/distil_bert_tokenizer.py:84\u001b[0m, in \u001b[0;36mDistilBertTokenizer.__init__\u001b[0;34m(self, vocabulary, lowercase, special_tokens_in_strings, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowercase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowercase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens_in_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecial_tokens_in_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/tokenizers/word_piece_tokenizer.py:365\u001b[0m, in \u001b[0;36mWordPieceTokenizer.__init__\u001b[0;34m(self, vocabulary, sequence_length, lowercase, strip_accents, split, split_on_cjk, suffix_indicator, oov_token, special_tokens, special_tokens_in_strings, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    352\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    364\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     \u001b[43massert_tf_text_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_int_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_string_dtype(dtype):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/tensor_utils.py:144\u001b[0m, in \u001b[0;36massert_tf_text_installed\u001b[0;34m(symbol_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires the `tensorflow-text` package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install with `pip install tensorflow-text`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n","\u001b[0;31mImportError\u001b[0m: DistilBertTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to reconstruct an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe class is missing a `from_config()` method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m     )\n\u001b[1;32m    723\u001b[0m \u001b[38;5;66;03m# Instantiate the class from its config inside a custom object scope\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# so that we can catch any custom objects that the config refers to.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py:236\u001b[0m, in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Error when deserializing class 'DistilBertTokenizer' using config={'name': 'distil_bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]'}.\n\nException encountered: DistilBertTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m preset\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistil_bert_base_en_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use a shorter sequence length.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_nlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDistilBertPreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_preset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessor_4_tweets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pretrained classifier.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mDistilBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(preset,\n\u001b[1;32m     12\u001b[0m                                                                preprocessor \u001b[38;5;241m=\u001b[39m preprocessor, \n\u001b[1;32m     13\u001b[0m                                                                num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/preprocessor.py:190\u001b[0m, in \u001b[0;36mPreprocessor.from_preset\u001b[0;34m(cls, preset, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m subclasses)\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmbiguous call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_preset()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound multiple possible subclasses \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease call `from_preset` on a subclass directly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         )\n\u001b[0;32m--> 190\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_serialized_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mload_preset_assets(preset)\n\u001b[1;32m    192\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/preset_utils.py:569\u001b[0m, in \u001b[0;36mload_serialized_object\u001b[0;34m(preset, config_file, config_overrides)\u001b[0m\n\u001b[1;32m    567\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(preset, config_file)\n\u001b[1;32m    568\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_overrides}\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to reconstruct an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe class is missing a `from_config()` method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m     )\n\u001b[1;32m    723\u001b[0m \u001b[38;5;66;03m# Instantiate the class from its config inside a custom object scope\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# so that we can catch any custom objects that the config refers to.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m custom_obj_scope \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mcustom_object_scope(custom_objects)\n","\u001b[0;31mTypeError\u001b[0m: <class 'keras_nlp.src.models.distil_bert.distil_bert_tokenizer.DistilBertTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_nlp.src.models.distil_bert.distil_bert_tokenizer', 'class_name': 'DistilBertTokenizer', 'config': {'name': 'distil_bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]'}, 'registered_name': 'keras_nlp>DistilBertTokenizer', 'assets': ['assets/tokenizer/vocabulary.txt'], 'weights': None}.\n\nException encountered: Error when deserializing class 'DistilBertTokenizer' using config={'name': 'distil_bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]'}.\n\nException encountered: DistilBertTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`."],"ename":"TypeError","evalue":"<class 'keras_nlp.src.models.distil_bert.distil_bert_tokenizer.DistilBertTokenizer'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras_nlp.src.models.distil_bert.distil_bert_tokenizer', 'class_name': 'DistilBertTokenizer', 'config': {'name': 'distil_bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]'}, 'registered_name': 'keras_nlp>DistilBertTokenizer', 'assets': ['assets/tokenizer/vocabulary.txt'], 'weights': None}.\n\nException encountered: Error when deserializing class 'DistilBertTokenizer' using config={'name': 'distil_bert_tokenizer', 'trainable': True, 'dtype': 'int32', 'vocabulary': None, 'sequence_length': None, 'lowercase': True, 'strip_accents': False, 'split': True, 'suffix_indicator': '##', 'oov_token': '[UNK]'}.\n\nException encountered: DistilBertTokenizer requires the `tensorflow-text` package. Please install with `pip install tensorflow-text`.","output_type":"error"}]},{"cell_type":"code","source":"import keras\nfrom keras.optimizers import Adam\n\n# Compile\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # 'binary_crossentropy',\n    optimizer=Adam(1e-5),  # Ensure correct instantiation\n    metrics=[\"accuracy\"]\n)\n\n# Fit\nhistory = classifier.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val, y_val)\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:39.096038Z","iopub.status.idle":"2024-07-28T15:36:39.096511Z","shell.execute_reply.started":"2024-07-28T15:36:39.096271Z","shell.execute_reply":"2024-07-28T15:36:39.096290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:39.098406Z","iopub.status.idle":"2024-07-28T15:36:39.099044Z","shell.execute_reply.started":"2024-07-28T15:36:39.098798Z","shell.execute_reply":"2024-07-28T15:36:39.098819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:39.260243Z","iopub.execute_input":"2024-07-28T15:36:39.260717Z","iopub.status.idle":"2024-07-28T15:36:39.305211Z","shell.execute_reply.started":"2024-07-28T15:36:39.260686Z","shell.execute_reply":"2024-07-28T15:36:39.304071Z"},"trusted":true},"execution_count":37,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m      3\u001b[0m displayConfusionMatrix(y_train, y_pred_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"],"ename":"NameError","evalue":"name 'classifier' is not defined","output_type":"error"}]},{"cell_type":"code","source":"y_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T15:36:39.778701Z","iopub.execute_input":"2024-07-28T15:36:39.779278Z","iopub.status.idle":"2024-07-28T15:36:39.820915Z","shell.execute_reply.started":"2024-07-28T15:36:39.779251Z","shell.execute_reply":"2024-07-28T15:36:39.819701Z"},"trusted":true},"execution_count":38,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m      3\u001b[0m displayConfusionMatrix(y_val, y_pred_val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"],"ename":"NameError","evalue":"name 'classifier' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# get predictions\ny_pred = classifier.predict(X_test)\n\n\n# create submission file\n#submission = pd.DataFrame({'id': df_test['id'], 'target': np.argmax(y_pred, axis=1)})\n#submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T08:31:24.939349Z","iopub.execute_input":"2024-07-25T08:31:24.939777Z","iopub.status.idle":"2024-07-25T08:31:41.922402Z","shell.execute_reply.started":"2024-07-25T08:31:24.939745Z","shell.execute_reply":"2024-07-25T08:31:41.920015Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 157ms/step\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1721896301.791771     149 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T08:31:41.924013Z","iopub.execute_input":"2024-07-25T08:31:41.924837Z","iopub.status.idle":"2024-07-25T08:31:41.940363Z","shell.execute_reply.started":"2024-07-25T08:31:41.924802Z","shell.execute_reply":"2024-07-25T08:31:41.939478Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"   id  target\n0   0       0\n1   2       0\n2   3       0\n3   9       0\n4  11       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T08:31:41.941671Z","iopub.execute_input":"2024-07-25T08:31:41.941962Z","iopub.status.idle":"2024-07-25T08:31:57.518435Z","shell.execute_reply.started":"2024-07-25T08:31:41.941919Z","shell.execute_reply":"2024-07-25T08:31:57.517675Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_submission.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T08:31:57.519661Z","iopub.execute_input":"2024-07-25T08:31:57.519957Z","iopub.status.idle":"2024-07-25T08:31:57.537040Z","shell.execute_reply.started":"2024-07-25T08:31:57.519912Z","shell.execute_reply":"2024-07-25T08:31:57.536231Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"                 id       target\ncount   3263.000000  3263.000000\nmean    5427.152927     0.350291\nstd     3146.427221     0.477134\nmin        0.000000     0.000000\n25%     2683.000000     0.000000\n50%     5500.000000     0.000000\n75%     8176.000000     1.000000\nmax    10875.000000     1.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3263.000000</td>\n      <td>3263.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5427.152927</td>\n      <td>0.350291</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3146.427221</td>\n      <td>0.477134</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2683.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5500.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8176.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10875.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_submission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T08:31:57.538192Z","iopub.execute_input":"2024-07-25T08:31:57.539137Z","iopub.status.idle":"2024-07-25T08:31:57.548069Z","shell.execute_reply.started":"2024-07-25T08:31:57.539111Z","shell.execute_reply":"2024-07-25T08:31:57.547302Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}